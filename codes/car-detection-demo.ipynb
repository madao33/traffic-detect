{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# ËΩ¶ËæÜÊ£ÄÊµãDemo\n",
    "\n",
    "Áõ¥Êé•Ë∞ÉÁî®[yolov5](https://github.com/ultralytics/yolov5)ËÆ≠ÁªÉÂ•ΩÁöÑÊ®°ÂûãÔºåËØ•ÁÆóÊ≥ïËøêÁî®ÂæàÁÆÄÂçïÔºå‰ª•‰∏ã‰ª£Á†Å‰ªÖ‰ªÖÂÅö‰∏Ä‰∏™ÊµãËØïÔºåÂêéÁª≠ÁöÑËØùËÆ≠ÁªÉÂèØ‰ª•Âè¶‰∏Ä‰ªΩ‰ª£Á†ÅÔºåÁÑ∂Âêé‰øùÂ≠òÊ®°ÂûãÔºåËøô‰∏™‰ª£Á†ÅÂèØ‰ª•Ë∞ÉÁî®Áõ∏Â∫îÁöÑÊ®°Âûã"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "# from yolov5.utils.general import non_max_suppression\n",
    "from deep_sort_pytorch.utils.parser import get_config\n",
    "from deep_sort_pytorch.deep_sort import DeepSort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(prediction, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False, multi_label=False,\n",
    "                        labels=()):\n",
    "    \"\"\"Runs Non-Maximum Suppression (NMS) on inference results\n",
    "\n",
    "    Returns:\n",
    "         list of detections, on (n,6) tensor per image [xyxy, conf, cls]\n",
    "    \"\"\"\n",
    "\n",
    "    nc = prediction.shape[2] - 5  # number of classes\n",
    "    xc = prediction[..., 4] > conf_thres  # candidates\n",
    "\n",
    "    # Settings\n",
    "    min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
    "    max_det = 300  # maximum number of detections per image\n",
    "    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
    "    time_limit = 10.0  # seconds to quit after\n",
    "    redundant = True  # require redundant detections\n",
    "    multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
    "    merge = False  # use merge-NMS\n",
    "\n",
    "    t = time.time()\n",
    "    output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
    "    for xi, x in enumerate(prediction):  # image index, image inference\n",
    "        # Apply constraints\n",
    "        # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
    "        x = x[xc[xi]]  # confidence\n",
    "\n",
    "        # Cat apriori labels if autolabelling\n",
    "        if labels and len(labels[xi]):\n",
    "            l = labels[xi]\n",
    "            v = torch.zeros((len(l), nc + 5), device=x.device)\n",
    "            v[:, :4] = l[:, 1:5]  # box\n",
    "            v[:, 4] = 1.0  # conf\n",
    "            v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
    "            x = torch.cat((x, v), 0)\n",
    "\n",
    "        # If none remain process next image\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # Compute conf\n",
    "        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
    "\n",
    "        # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
    "        box = xywh2xyxy(x[:, :4])\n",
    "\n",
    "        # Detections matrix nx6 (xyxy, conf, cls)\n",
    "        if multi_label:\n",
    "            i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
    "            x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
    "        else:  # best class only\n",
    "            conf, j = x[:, 5:].max(1, keepdim=True)\n",
    "            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
    "\n",
    "        # Filter by class\n",
    "        if classes is not None:\n",
    "            x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
    "\n",
    "        # Apply finite constraint\n",
    "        # if not torch.isfinite(x).all():\n",
    "        #     x = x[torch.isfinite(x).all(1)]\n",
    "\n",
    "        # Check shape\n",
    "        n = x.shape[0]  # number of boxes\n",
    "        if not n:  # no boxes\n",
    "            continue\n",
    "        elif n > max_nms:  # excess boxes\n",
    "            x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
    "\n",
    "        # Batched NMS\n",
    "        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
    "        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
    "        i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
    "        if i.shape[0] > max_det:  # limit detections\n",
    "            i = i[:max_det]\n",
    "        if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
    "            # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
    "            iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
    "            weights = iou * scores[None]  # box weights\n",
    "            x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
    "            if redundant:\n",
    "                i = i[iou.sum(1) > 1]  # require redundancy\n",
    "\n",
    "        output[xi] = x[i]\n",
    "        if (time.time() - t) > time_limit:\n",
    "            print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
    "            break  # time limit exceeded\n",
    "\n",
    "    return output"
   ]
  },
  {
   "source": [
    "## ÂàùÂßãÂåñÊ®°Âûã\n",
    "\n",
    "Âà©Áî®`PyTorch Hub`Âä†ËΩΩ **YOLOv5** Ê®°ÂûãÔºå‰∏∫‰∫ÜÊñπ‰æøËøêË°åÔºåÂ∞Ü`yolov5s.pt` Êã∑Ë¥ùÂà∞ËØ•‰ª£Á†ÅÁöÑÂêå‰∏ÄÁ∫ßÁõÆÂΩï‰∏ãÔºåÊñπ‰æøÁõ¥Êé•Ë∞ÉÁî®ÔºåÁúÅÂéªÁΩëÁªúÂä†ËΩΩÁöÑÊó∂Èó¥ÔºåÊàëËøôÈáåÁöÑÁéØÂ¢ÉÂ¶Ç‰∏ãÔºö"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'1.8.0+cu111'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linux‰∏ãÊü•ÁúãcudaÁâàÊú¨\n",
    "# !nvcc -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cache found in /home/madao/.cache/torch/hub/ultralytics_yolov5_master\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  1    156928  models.common.C3                        [128, 128, 3]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  1    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
      "  9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model Summary: 283 layers, 7276605 parameters, 7276605 gradients, 17.1 GFLOPS\n",
      "\n",
      "YOLOv5 üöÄ  torch 1.8.0+cu111 CUDA:0 (GeForce GTX 1660, 5941.5MB)\n",
      "\n",
      "Adding autoShape... \n"
     ]
    }
   ],
   "source": [
    "# model load\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')"
   ]
  },
  {
   "source": [
    "## yolov5È¢ÑÊµã"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "img_path = \"../datasets/objectDetection/testing_images/\"\n",
    "imgfiles = os.listdir(img_path)\n",
    "len(imgfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "imgs = [cv2.imread(img_path + imgfile)[:, :, ::-1] for imgfile in imgfiles]\n",
    "len(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomÊµãËØï\n",
    "chose = np.random.randint(len(imgs))\n",
    "result = model(imgs[chose], size=640)\n",
    "# print(result.xyxy)\n",
    "# result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model(imgs[1:20], size=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = results.names\n",
    "# results.xyxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Âçï‰∏™ÂõæÁâáÊµãËØï\n",
    "# boxes = results.xyxy[0].cpu().numpy()\n",
    "# img_ = imgs[1].copy()\n",
    "# cv2.rectangle(img_, (boxes[0, 0], boxes[0, 1]), (boxes[0, 2], boxes[0, 3]), (0, 255, 0), 2)\n",
    "# cv2.imshow('', img_)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "source": [
    "## Ê£ÄÊµãËßÜÈ¢ëÊµãËØï"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = '../datasets/video/cars2.mp4' # ËßÜÈ¢ëË∑ØÂæÑ\n",
    "video = cv2.VideoCapture(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addBox(img, boxes):\n",
    "    for box in boxes:\n",
    "        if int(box[5]) == 2:\n",
    "            cv2.rectangle(img, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ÁõÆÊ†áÊ£ÄÊµãËΩ¶ËæÜÊµãËØï\n",
    "# while True:\n",
    "#     start_time = time.time()\n",
    "#     rc, image = video.read()\n",
    "#     if image is None:\n",
    "#         cv2.destroyAllWindows()\n",
    "#         break\n",
    "#     resultImage = image\n",
    "#     boxes = model(resultImage).xyxy[0].cpu().numpy()\n",
    "#     addBox(resultImage, boxes)\n",
    "#     cv2.imshow('result', resultImage)\n",
    "    \n",
    "#     # ÊåâqÈÄÄÂá∫\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         cv2.destroyAllWindows()\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_rel(xyxy):\n",
    "    \"\"\"\" Calculates the relative bounding box from absolute pixel values. \"\"\"\n",
    "    bbox_left = min([xyxy[0], xyxy[2]])\n",
    "    bbox_top = min([xyxy[1], xyxy[3]])\n",
    "    bbox_w = abs(xyxy[0] - xyxy[2])\n",
    "    bbox_h = abs(xyxy[1] - xyxy[3])\n",
    "    x_c = (bbox_left + bbox_w / 2)\n",
    "    y_c = (bbox_top + bbox_h / 2)\n",
    "    w = bbox_w\n",
    "    h = bbox_h\n",
    "    return x_c, y_c, w, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\n",
    "def compute_color_for_labels(label):\n",
    "    \"\"\"\n",
    "    Simple function that adds fixed color depending on the class\n",
    "    \"\"\"\n",
    "    color = [int((p * (label ** 2 - label + 1)) % 255) for p in palette]\n",
    "    return tuple(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkTraffic(speed=None, vthresh = 10):\n",
    "    \"\"\"\n",
    "    Ê†πÊçÆÈÄüÂ∫¶Â≠óÂÖ∏Ê£ÄÊü•ÊòØÂê¶Êã•Â†µÔºåËøîÂõû‰∏∫TrueÂàôË°®Á§∫Êã•Â†µÔºåÂê¶Âàô‰∏∫Ê≠£Â∏∏\n",
    "    \"\"\"\n",
    "    if speed != None:\n",
    "        # Ëé∑ÂèñÊúâÈÄüÂ∫¶ÁöÑËΩ¶ËæÜÊï∞ÁõÆ\n",
    "        carnum = len(speed)\n",
    "        lowcars = 0\n",
    "        for k, v in speed.items():\n",
    "            if v < vthresh:\n",
    "                lowcars += 1\n",
    "        if lowcars > carnum * 0.5:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(img, bbox, identities=None, speed=None, offset=(0, 0), line=(0.7, 0.8), framecounter=25, status=False):\n",
    "    speedval = 0\n",
    "    height, width, _cha = img.shape\n",
    "    for i, box in enumerate(bbox):\n",
    "        x1, y1, x2, y2 = [int(i) for i in box]\n",
    "        x1 += offset[0]\n",
    "        x2 += offset[0]\n",
    "        y1 += offset[1]\n",
    "        y2 += offset[1]\n",
    "        center_y = int((y1+y2)/2)\n",
    "        center_x = int((x1+x2)/2)\n",
    "        # box text and bar\n",
    "        id = int(identities[i]) if identities is not None else 0\n",
    "        color = compute_color_for_labels(id)\n",
    "        label = '{}{:d}'.format(\"\", id)\n",
    "        if speed != None and id in speed:\n",
    "            label += \"-v-\" + str(speed[id])\n",
    "        t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color, 3)\n",
    "        cv2.rectangle(\n",
    "            img, (x1, y1), (x1 + t_size[0] + 3, y1 + t_size[1] + 4), color, -1)\n",
    "        cv2.putText(img, label, (x1, y1 +\n",
    "                                 t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [255, 255, 255], 1)\n",
    "\n",
    "        cv2.line(img, (0, int(line[0]*height)), (width, int(line[0] * height)), [255, 0, 0], 1)\n",
    "        cv2.line(img, (0, int(line[1]*height)), (width, int(line[1] * height)), [255, 0, 0], 1)\n",
    "        if center_y < line[1] * height and center_y > line[0] * height:\n",
    "            cv2.drawMarker(img, (center_x, center_y), [0, 255, 255], markerType=3)\n",
    "    cv2.putText(img, \"f:\" + str(framecounter), (20, 20), cv2.FONT_HERSHEY_PLAIN, 1, [0, 255, 0], 2)\n",
    "    cv2.putText(img, 'traffic:' + str(status), (20, 40), cv2.FONT_HERSHEY_PLAIN, 1, [0, 255, 0], 2)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdict(outputs):\n",
    "    centerdict = dict()\n",
    "    for i in range(len(outputs)):\n",
    "        x = outputs[i, 0] + outputs[i, 1]\n",
    "        y = outputs[i, 2] + outputs[i, 3]\n",
    "        id = outputs[i, -1]\n",
    "        centerdict[id] = np.array([x/2, y/2])\n",
    "    return centerdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getspeed(preoutputs, outputs, framecounter=10, fps=30, offset=1, height=1080, width=1920):\n",
    "    pre = getdict(preoutputs)\n",
    "    now = getdict(outputs)\n",
    "    # print('dict', pre)\n",
    "    # print('dict', now)\n",
    "    speed = dict()\n",
    "    for id, center in now.items():\n",
    "        # ËÆ°ÁÆó‰∏äÊ¨°Ê£ÄÊµã‰∏§ËÄÖ‰πãÈó¥ÊñπÊ°Ü‰∏≠ÂøÉÁöÑÊ¨ßÊ∞èË∑ùÁ¶ª\n",
    "        if id in pre:\n",
    "            speed[id] = (pre[id] - now[id])**2\n",
    "            speed[id] = np.around(np.sqrt(speed[id].sum())/framecounter*fps*offset/ center[1]**2 *height, decimals=1)\n",
    "        else:\n",
    "            speed[id] = np.around(0, decimals=1)\n",
    "    return speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateSpeed(prespeed, speed):\n",
    "    res = dict()\n",
    "    for id, v in speed.items():\n",
    "        if id in prespeed:\n",
    "            res[id] = np.around((prespeed[id] + speed[id])/2, decimals=1)\n",
    "        else:\n",
    "            res[id] = speed[id]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/madao/Documents/partjob/B10078/codes/deep_sort_pytorch/utils/parser.py:24: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n  self.update(yaml.load(fo.read()))\nLoading weights from /home/madao/Documents/partjob/B10078/codes/deep_sort_pytorch/deep_sort/deep/checkpoint/ckpt.t7... Done!\n"
     ]
    }
   ],
   "source": [
    "framecounter = 0\n",
    "speed = dict()\n",
    "fscount = dict()\n",
    "ages = dict()\n",
    "offset = 10000\n",
    "high = 0.6\n",
    "low = 0.5\n",
    "coef = 20\n",
    "vthresh = 20\n",
    "status = False\n",
    "# ËØªÂèñÁöÑËßÜÈ¢ëË∑ØÂæÑ\n",
    "video_path = '../datasets/video/cars3.mp4'\n",
    "video = cv2.VideoCapture(video_path)\n",
    "fps = video.get(5)\n",
    "# ÂàùÂßãÂåñdeepsort\n",
    "cfg = get_config()\n",
    "cfg.merge_from_file(\"deep_sort_pytorch/configs/deep_sort.yaml\")\n",
    "deepsort = DeepSort(cfg.DEEPSORT.REID_CKPT,\n",
    "                    max_dist=cfg.DEEPSORT.MAX_DIST, min_confidence=cfg.DEEPSORT.MIN_CONFIDENCE,\n",
    "                    nms_max_overlap=cfg.DEEPSORT.NMS_MAX_OVERLAP, max_iou_distance=cfg.DEEPSORT.MAX_IOU_DISTANCE,\n",
    "                    max_age=cfg.DEEPSORT.MAX_AGE, n_init=cfg.DEEPSORT.N_INIT, nn_budget=cfg.DEEPSORT.NN_BUDGET,\n",
    "                    use_cuda=True)\n",
    "while True:\n",
    "    # ËØªÂèñËßÜÈ¢ë\n",
    "    rc, image = video.read()\n",
    "    framecounter += 1   # Â∏ßÊï∞ËÆ°Êï∞Âä†‰∏Ä\n",
    "    \n",
    "    if image is None:\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "    # ÊåâqÈÄÄÂá∫\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "    height, width, _channel = image.shape\n",
    "    resimg = image.copy()\n",
    "    res = model(resimg)\n",
    "    # res.show()\n",
    "    pred = res.xyxy\n",
    "    # print(pred)\n",
    "    for i, det in enumerate(pred):\n",
    "        # print(i, \":\", det)\n",
    "        if det is not None and len(det):\n",
    "            bbox_xywh = []\n",
    "            confs = []\n",
    "            for i in range(len(det)):\n",
    "                xyxy = det[i,0:4]\n",
    "                conf = det[i,4]\n",
    "                label = det[i, -1]\n",
    "                # Ê£ÄÊµãÂà∞ËΩ¶ËæÜ\n",
    "                if int(label)==2:\n",
    "                    x_c, y_c, bbox_w, bbox_h = bbox_rel(xyxy)\n",
    "                    obj = [x_c, y_c, bbox_w, bbox_h]\n",
    "                    bbox_xywh.append(obj)\n",
    "                    confs.append([conf])\n",
    "\n",
    "            xywhs = torch.Tensor(bbox_xywh)\n",
    "            confss = torch.Tensor(confs)\n",
    "            outputs = deepsort.update(xywhs, confss, resimg)\n",
    "\n",
    "            if len(outputs) > 0:\n",
    "                bbox_xywh = outputs[:, :4]\n",
    "                indentities = outputs[:, -1]\n",
    "                \n",
    "                for i, id in enumerate(indentities):\n",
    "                    x1, y1, x2, y2 = [int(i) for i in bbox_xywh[i]]\n",
    "                    x1 += 0\n",
    "                    x2 += 0\n",
    "                    y1 += 0\n",
    "                    y2 += 0\n",
    "                    center_y = int((y1+y2)/2)\n",
    "                    center_x = int((x1+x2)/2)\n",
    "                    # print(center_x, center_y)\n",
    "                    if int(center_y) < int(high*height) and int(center_y) > int(low*height):\n",
    "                        \n",
    "                        if id not in fscount:\n",
    "                            # print('first checked:', id, int(center_y), '(', int(low*height), ', ', int(high*height), ')', framecounter)\n",
    "                            fscount[id] = framecounter\n",
    "                        \n",
    "                    else:\n",
    "                        if id in fscount:\n",
    "                            if id not in ages:\n",
    "                                ages[id] = framecounter\n",
    "                            else:\n",
    "                                if framecounter - ages[id] < 5:\n",
    "                                    pass\n",
    "                                else:\n",
    "                                    speed[id] = int((high-low)*height/(framecounter - fscount[id] - 5) * coef)\n",
    "                                    status = checkTraffic(speed, vthresh)\n",
    "                                    # print('second checked:', id, int(center_y), '(', int(low*height), ', ', int(high*height), ')', 'speed:', speed[id], framecounter)\n",
    "                                    fscount.pop(id)\n",
    "                                    ages.pop(id)\n",
    "                        \n",
    "                    \n",
    "                draw_boxes(resimg, bbox_xywh, indentities, speed, line=(low, high), framecounter=framecounter, status=status)\n",
    "        else:\n",
    "            deepsort.increment_ages()\n",
    "    # time.sleep(3)\n",
    "    cv2.imshow('', resimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}